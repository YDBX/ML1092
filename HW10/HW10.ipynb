{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "「hw10_adversarial_attack.ipynb」的副本",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-n2e0BkhEKS"
      },
      "source": [
        "# **Homework 10 - Adversarial Attack**\n",
        "\n",
        "Slides: https://reurl.cc/v5kXkk\n",
        "\n",
        "Videos:\n",
        "\n",
        "TA: ntu-ml-2021spring-ta@googlegroups.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RX7iRXrhMA_"
      },
      "source": [
        "## Enviroment & Download\n",
        "\n",
        "We make use of [pytorchcv](https://pypi.org/project/pytorchcv/) to obtain CIFAR-10 pretrained model, so we need to set up the enviroment first. We also need to download the data (200 images) which we want to attack."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4Lw7urignqP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55cd941f-ca3e-450a-e2bb-997e4dc55cf0"
      },
      "source": [
        "# set up environment\n",
        "!pip install pytorchcv\n",
        "\n",
        "# download\n",
        "!gdown --id 1fHi1ko7wr80wXkXpqpqpOxuYH1mClXoX -O data.zip\n",
        "\n",
        "# unzip\n",
        "!unzip ./data.zip\n",
        "!rm ./data.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorchcv\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/44/74/e5dae0875679d296fa9a3833041699cee9222e2d3dd1f9ae1ded050b5672/pytorchcv-0.0.65-py2.py3-none-any.whl (527kB)\n",
            "\r\u001b[K     |▋                               | 10kB 17.1MB/s eta 0:00:01\r\u001b[K     |█▎                              | 20kB 23.6MB/s eta 0:00:01\r\u001b[K     |█▉                              | 30kB 26.4MB/s eta 0:00:01\r\u001b[K     |██▌                             | 40kB 27.4MB/s eta 0:00:01\r\u001b[K     |███                             | 51kB 29.0MB/s eta 0:00:01\r\u001b[K     |███▊                            | 61kB 31.7MB/s eta 0:00:01\r\u001b[K     |████▍                           | 71kB 31.7MB/s eta 0:00:01\r\u001b[K     |█████                           | 81kB 32.8MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 92kB 33.9MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 102kB 34.8MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 112kB 34.8MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 122kB 34.8MB/s eta 0:00:01\r\u001b[K     |████████                        | 133kB 34.8MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 143kB 34.8MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 153kB 34.8MB/s eta 0:00:01\r\u001b[K     |██████████                      | 163kB 34.8MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 174kB 34.8MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 184kB 34.8MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 194kB 34.8MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 204kB 34.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 215kB 34.8MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 225kB 34.8MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 235kB 34.8MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 245kB 34.8MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 256kB 34.8MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 266kB 34.8MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 276kB 34.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 286kB 34.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 296kB 34.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 307kB 34.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 317kB 34.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 327kB 34.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 337kB 34.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 348kB 34.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 358kB 34.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 368kB 34.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 378kB 34.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 389kB 34.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 399kB 34.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 409kB 34.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 419kB 34.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 430kB 34.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 440kB 34.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 450kB 34.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 460kB 34.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 471kB 34.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 481kB 34.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 491kB 34.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 501kB 34.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 512kB 34.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 522kB 34.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 532kB 34.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorchcv) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorchcv) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorchcv) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorchcv) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pytorchcv) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorchcv) (2020.12.5)\n",
            "Installing collected packages: pytorchcv\n",
            "Successfully installed pytorchcv-0.0.65\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1fHi1ko7wr80wXkXpqpqpOxuYH1mClXoX\n",
            "To: /content/data.zip\n",
            "100% 490k/490k [00:00<00:00, 7.77MB/s]\n",
            "Archive:  ./data.zip\n",
            "   creating: data/\n",
            "   creating: data/deer/\n",
            " extracting: data/deer/deer13.png    \n",
            " extracting: data/deer/deer6.png     \n",
            " extracting: data/deer/deer11.png    \n",
            " extracting: data/deer/deer2.png     \n",
            " extracting: data/deer/deer10.png    \n",
            " extracting: data/deer/deer16.png    \n",
            " extracting: data/deer/deer9.png     \n",
            " extracting: data/deer/deer20.png    \n",
            " extracting: data/deer/deer15.png    \n",
            " extracting: data/deer/deer19.png    \n",
            " extracting: data/deer/deer5.png     \n",
            " extracting: data/deer/deer14.png    \n",
            " extracting: data/deer/deer4.png     \n",
            " extracting: data/deer/deer8.png     \n",
            " extracting: data/deer/deer12.png    \n",
            " extracting: data/deer/deer1.png     \n",
            " extracting: data/deer/deer7.png     \n",
            " extracting: data/deer/deer17.png    \n",
            " extracting: data/deer/deer18.png    \n",
            " extracting: data/deer/deer3.png     \n",
            "   creating: data/horse/\n",
            " extracting: data/horse/horse9.png   \n",
            " extracting: data/horse/horse1.png   \n",
            " extracting: data/horse/horse16.png  \n",
            " extracting: data/horse/horse15.png  \n",
            " extracting: data/horse/horse19.png  \n",
            " extracting: data/horse/horse14.png  \n",
            " extracting: data/horse/horse10.png  \n",
            " extracting: data/horse/horse7.png   \n",
            " extracting: data/horse/horse2.png   \n",
            " extracting: data/horse/horse6.png   \n",
            " extracting: data/horse/horse20.png  \n",
            " extracting: data/horse/horse5.png   \n",
            " extracting: data/horse/horse18.png  \n",
            " extracting: data/horse/horse12.png  \n",
            " extracting: data/horse/horse13.png  \n",
            " extracting: data/horse/horse17.png  \n",
            " extracting: data/horse/horse4.png   \n",
            " extracting: data/horse/horse11.png  \n",
            " extracting: data/horse/horse8.png   \n",
            " extracting: data/horse/horse3.png   \n",
            "   creating: data/ship/\n",
            " extracting: data/ship/ship10.png    \n",
            " extracting: data/ship/ship14.png    \n",
            " extracting: data/ship/ship9.png     \n",
            " extracting: data/ship/ship20.png    \n",
            " extracting: data/ship/ship5.png     \n",
            " extracting: data/ship/ship8.png     \n",
            " extracting: data/ship/ship19.png    \n",
            " extracting: data/ship/ship16.png    \n",
            " extracting: data/ship/ship13.png    \n",
            " extracting: data/ship/ship6.png     \n",
            " extracting: data/ship/ship17.png    \n",
            " extracting: data/ship/ship1.png     \n",
            " extracting: data/ship/ship12.png    \n",
            " extracting: data/ship/ship2.png     \n",
            " extracting: data/ship/ship3.png     \n",
            " extracting: data/ship/ship15.png    \n",
            " extracting: data/ship/ship4.png     \n",
            " extracting: data/ship/ship7.png     \n",
            " extracting: data/ship/ship11.png    \n",
            " extracting: data/ship/ship18.png    \n",
            "   creating: data/frog/\n",
            " extracting: data/frog/frog10.png    \n",
            " extracting: data/frog/frog4.png     \n",
            " extracting: data/frog/frog5.png     \n",
            " extracting: data/frog/frog20.png    \n",
            " extracting: data/frog/frog15.png    \n",
            " extracting: data/frog/frog3.png     \n",
            " extracting: data/frog/frog1.png     \n",
            " extracting: data/frog/frog14.png    \n",
            " extracting: data/frog/frog2.png     \n",
            " extracting: data/frog/frog19.png    \n",
            " extracting: data/frog/frog7.png     \n",
            " extracting: data/frog/frog11.png    \n",
            " extracting: data/frog/frog17.png    \n",
            " extracting: data/frog/frog18.png    \n",
            " extracting: data/frog/frog12.png    \n",
            " extracting: data/frog/frog16.png    \n",
            " extracting: data/frog/frog8.png     \n",
            " extracting: data/frog/frog13.png    \n",
            " extracting: data/frog/frog6.png     \n",
            " extracting: data/frog/frog9.png     \n",
            "   creating: data/airplane/\n",
            " extracting: data/airplane/airplane3.png  \n",
            " extracting: data/airplane/airplane4.png  \n",
            " extracting: data/airplane/airplane2.png  \n",
            " extracting: data/airplane/airplane9.png  \n",
            " extracting: data/airplane/airplane20.png  \n",
            " extracting: data/airplane/airplane18.png  \n",
            " extracting: data/airplane/airplane19.png  \n",
            " extracting: data/airplane/airplane10.png  \n",
            " extracting: data/airplane/airplane6.png  \n",
            " extracting: data/airplane/airplane13.png  \n",
            " extracting: data/airplane/airplane16.png  \n",
            " extracting: data/airplane/airplane14.png  \n",
            " extracting: data/airplane/airplane11.png  \n",
            " extracting: data/airplane/airplane1.png  \n",
            " extracting: data/airplane/airplane17.png  \n",
            " extracting: data/airplane/airplane7.png  \n",
            " extracting: data/airplane/airplane15.png  \n",
            " extracting: data/airplane/airplane5.png  \n",
            " extracting: data/airplane/airplane8.png  \n",
            " extracting: data/airplane/airplane12.png  \n",
            "   creating: data/bird/\n",
            " extracting: data/bird/bird9.png     \n",
            " extracting: data/bird/bird12.png    \n",
            " extracting: data/bird/bird10.png    \n",
            " extracting: data/bird/bird11.png    \n",
            " extracting: data/bird/bird5.png     \n",
            " extracting: data/bird/bird8.png     \n",
            " extracting: data/bird/bird4.png     \n",
            " extracting: data/bird/bird3.png     \n",
            " extracting: data/bird/bird7.png     \n",
            " extracting: data/bird/bird18.png    \n",
            " extracting: data/bird/bird14.png    \n",
            " extracting: data/bird/bird13.png    \n",
            " extracting: data/bird/bird2.png     \n",
            " extracting: data/bird/bird15.png    \n",
            " extracting: data/bird/bird17.png    \n",
            " extracting: data/bird/bird19.png    \n",
            " extracting: data/bird/bird16.png    \n",
            " extracting: data/bird/bird6.png     \n",
            " extracting: data/bird/bird20.png    \n",
            " extracting: data/bird/bird1.png     \n",
            "   creating: data/cat/\n",
            " extracting: data/cat/cat6.png       \n",
            " extracting: data/cat/cat1.png       \n",
            " extracting: data/cat/cat7.png       \n",
            " extracting: data/cat/cat19.png      \n",
            " extracting: data/cat/cat5.png       \n",
            " extracting: data/cat/cat9.png       \n",
            " extracting: data/cat/cat17.png      \n",
            " extracting: data/cat/cat2.png       \n",
            " extracting: data/cat/cat16.png      \n",
            " extracting: data/cat/cat10.png      \n",
            " extracting: data/cat/cat4.png       \n",
            " extracting: data/cat/cat18.png      \n",
            " extracting: data/cat/cat13.png      \n",
            " extracting: data/cat/cat11.png      \n",
            " extracting: data/cat/cat20.png      \n",
            " extracting: data/cat/cat15.png      \n",
            " extracting: data/cat/cat8.png       \n",
            " extracting: data/cat/cat14.png      \n",
            " extracting: data/cat/cat3.png       \n",
            " extracting: data/cat/cat12.png      \n",
            "   creating: data/automobile/\n",
            " extracting: data/automobile/automobile17.png  \n",
            " extracting: data/automobile/automobile11.png  \n",
            " extracting: data/automobile/automobile5.png  \n",
            " extracting: data/automobile/automobile10.png  \n",
            " extracting: data/automobile/automobile20.png  \n",
            " extracting: data/automobile/automobile2.png  \n",
            " extracting: data/automobile/automobile6.png  \n",
            " extracting: data/automobile/automobile1.png  \n",
            " extracting: data/automobile/automobile19.png  \n",
            " extracting: data/automobile/automobile7.png  \n",
            " extracting: data/automobile/automobile16.png  \n",
            " extracting: data/automobile/automobile3.png  \n",
            " extracting: data/automobile/automobile14.png  \n",
            " extracting: data/automobile/automobile12.png  \n",
            " extracting: data/automobile/automobile9.png  \n",
            " extracting: data/automobile/automobile4.png  \n",
            " extracting: data/automobile/automobile8.png  \n",
            " extracting: data/automobile/automobile13.png  \n",
            " extracting: data/automobile/automobile18.png  \n",
            " extracting: data/automobile/automobile15.png  \n",
            "   creating: data/dog/\n",
            " extracting: data/dog/dog9.png       \n",
            " extracting: data/dog/dog2.png       \n",
            " extracting: data/dog/dog15.png      \n",
            " extracting: data/dog/dog8.png       \n",
            " extracting: data/dog/dog3.png       \n",
            " extracting: data/dog/dog19.png      \n",
            " extracting: data/dog/dog12.png      \n",
            " extracting: data/dog/dog7.png       \n",
            " extracting: data/dog/dog17.png      \n",
            " extracting: data/dog/dog11.png      \n",
            " extracting: data/dog/dog16.png      \n",
            " extracting: data/dog/dog20.png      \n",
            " extracting: data/dog/dog4.png       \n",
            " extracting: data/dog/dog5.png       \n",
            " extracting: data/dog/dog14.png      \n",
            " extracting: data/dog/dog18.png      \n",
            " extracting: data/dog/dog10.png      \n",
            " extracting: data/dog/dog1.png       \n",
            " extracting: data/dog/dog13.png      \n",
            " extracting: data/dog/dog6.png       \n",
            "   creating: data/truck/\n",
            " extracting: data/truck/truck1.png   \n",
            " extracting: data/truck/truck18.png  \n",
            " extracting: data/truck/truck9.png   \n",
            " extracting: data/truck/truck4.png   \n",
            " extracting: data/truck/truck14.png  \n",
            " extracting: data/truck/truck8.png   \n",
            " extracting: data/truck/truck12.png  \n",
            " extracting: data/truck/truck15.png  \n",
            " extracting: data/truck/truck2.png   \n",
            " extracting: data/truck/truck5.png   \n",
            " extracting: data/truck/truck3.png   \n",
            " extracting: data/truck/truck10.png  \n",
            " extracting: data/truck/truck17.png  \n",
            " extracting: data/truck/truck20.png  \n",
            " extracting: data/truck/truck19.png  \n",
            " extracting: data/truck/truck13.png  \n",
            " extracting: data/truck/truck7.png   \n",
            " extracting: data/truck/truck6.png   \n",
            "  inflating: data/truck/truck16.png  \n",
            " extracting: data/truck/truck11.png  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZD3aTZHPKIh",
        "outputId": "6e030862-2efd-4ef3-f82d-550277f41344"
      },
      "source": [
        "!nvidia-smi -L"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-758b4681-8bcd-0bcf-d61e-05ba2646f497)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkQQf0l1hbBs"
      },
      "source": [
        "## Global Settings\n",
        "\n",
        "* $\\epsilon$ is fixed to be 8. But on **Data section**, we will first apply transforms on raw pixel value (0-255 scale) **by ToTensor (to 0-1 scale)** and then **Normalize (subtract mean divide std)**. $\\epsilon$ should be set to $\\frac{8}{255 * std}$ during attack.\n",
        "\n",
        "* Explaination (optional)\n",
        "    * Denote the first pixel of original image as $p$, and the first pixel of adversarial image as $a$.\n",
        "    * The $\\epsilon$ constraints tell us $\\left| p-a \\right| <= 8$.\n",
        "    * ToTensor() can be seen as a function where $T(x) = x/255$.\n",
        "    * Normalize() can be seen as a function where $N(x) = (x-mean)/std$ where $mean$ and $std$ are constants.\n",
        "    * After applying ToTensor() and Normalize() on $p$ and $a$, the constraint becomes $\\left| N(T(p))-N(T(a)) \\right| = \\left| \\frac{\\frac{p}{255}-mean}{std}-\\frac{\\frac{a}{255}-mean}{std} \\right| = \\frac{1}{255 * std} \\left| p-a \\right| <= \\frac{8}{255 * std}.$\n",
        "    * So, we should set $\\epsilon$ to be $\\frac{8}{255 * std}$ after ToTensor() and Normalize()."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACghc_tsg2vE"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "batch_size = 8\n",
        "p = 1 / 10\n",
        "\n",
        "# the mean and std are the calculated statistics from cifar_10 dataset\n",
        "cifar_10_mean = (0.491, 0.482, 0.447) # mean for the three channels of cifar_10 images\n",
        "cifar_10_std = (0.202, 0.199, 0.201) # std for the three channels of cifar_10 images\n",
        "\n",
        "# convert mean and std to 3-dimensional tensors for future operations\n",
        "mean = torch.tensor(cifar_10_mean).to(device).view(3, 1, 1)\n",
        "std = torch.tensor(cifar_10_std).to(device).view(3, 1, 1)\n",
        "\n",
        "epsilon = 8/255/std\n",
        "# TODO: iterative fgsm attack\n",
        "# alpha (step size) can be decided by yourself\n",
        "alpha = 0.8/255/std\n",
        "\n",
        "root = './data' # directory for storing benign images\n",
        "# benign images: images which do not contain adversarial perturbations\n",
        "# adversarial images: images which include adversarial perturbations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhBJBAlKherZ"
      },
      "source": [
        "## Data\n",
        "\n",
        "Construct dataset and dataloader from root directory. Note that we store the filename of each image for future usage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXpRAHz0hkDt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09e7f90c-c75a-4db2-b6ab-b415ddcdfce5"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision.transforms import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(cifar_10_mean, cifar_10_std)\n",
        "])\n",
        "\n",
        "class AdvDataset(Dataset):\n",
        "    def __init__(self, data_dir, transform):\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "        self.names = []\n",
        "        '''\n",
        "        data_dir\n",
        "        ├── class_dir\n",
        "        │   ├── class1.png\n",
        "        │   ├── ...\n",
        "        │   ├── class20.png\n",
        "        '''\n",
        "        for i, class_dir in enumerate(sorted(glob.glob(f'{data_dir}/*'))):\n",
        "            images = sorted(glob.glob(f'{class_dir}/*'))\n",
        "            self.images += images\n",
        "            self.labels += ([i] * len(images))\n",
        "            self.names += [os.path.relpath(imgs, data_dir) for imgs in images]\n",
        "        self.transform = transform\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.transform(Image.open(self.images[idx]))\n",
        "        label = self.labels[idx]\n",
        "        return image, label\n",
        "    def __getname__(self):\n",
        "        return self.names\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "adv_set = AdvDataset(root, transform=transform)\n",
        "adv_names = adv_set.__getname__()\n",
        "adv_loader = DataLoader(adv_set, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f'number of images = {adv_set.__len__()}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of images = 200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnszlTsYrTQZ"
      },
      "source": [
        "## Utils -- Benign Images Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5c_zZLzkrceE"
      },
      "source": [
        "# to evaluate the performance of model on benign images\n",
        "def epoch_benign(models, loader, loss_fn):\n",
        "    for model in models:\n",
        "      model.eval()\n",
        "    train_acc, train_loss = 0.0, 0.0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        yps = list()\n",
        "        for model in models:\n",
        "          yps.append(model(x))\n",
        "        for index, yp in enumerate(yps):\n",
        "          if index == 0:\n",
        "            loss = loss_fn(yp, y) * p\n",
        "          else:\n",
        "            loss += loss_fn(yp, y) * p\n",
        "          train_acc += (yp.argmax(dim=1) == y).sum().item() * p\n",
        "        train_loss += loss.item() * x.shape[0]\n",
        "    return train_acc / len(loader.dataset), train_loss / len(loader.dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YJxK7YehqQy"
      },
      "source": [
        "## Utils -- Attack Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_1wKfKyhrQW"
      },
      "source": [
        "# perform fgsm attack\n",
        "def fgsm(models, x, y, loss_fn, epsilon=epsilon):\n",
        "    x_adv = x.detach().clone() # initialize x_adv as original benign image x\n",
        "    x_adv.requires_grad = True # need to obtain gradient of x_adv, thus set required grad\n",
        "\n",
        "    for index, model in enumerate(models):\n",
        "        if index == 0:\n",
        "            loss = loss_fn(model(x_adv), y) * p\n",
        "        else:\n",
        "            loss += loss_fn(model(x_adv), y) * p\n",
        "    loss.backward() # calculate gradient\n",
        "    # fgsm: use gradient ascent on x_adv to maximize loss\n",
        "    x_adv = x_adv + epsilon * x_adv.grad.detach().sign()\n",
        "    return x_adv\n",
        "    \n",
        "\n",
        "# TODO: perform iterative fgsm attack\n",
        "# set alpha as the step size in Global Settings section\n",
        "# alpha and num_iter can be decided by yourself\n",
        "def ifgsm(models, x, y, loss_fn, epsilon=epsilon, alpha=alpha, num_iter=100):\n",
        "    # initialize x_adv as original benign image x\n",
        "    x_adv = x.detach().clone()\n",
        "    # write a loop of num_iter to represent the iterative times\n",
        "    # for each loop\n",
        "    for i in range(num_iter):\n",
        "        # call fgsm with (epsilon = alpha) to obtain new x_adv\n",
        "        x_adv = fgsm(models, x_adv, y, loss_fn, alpha)\n",
        "        # clip new x_adv back to [x-epsilon, x+epsilon]\n",
        "        x_adv = torch.min(torch.max(x_adv, x-epsilon), x+epsilon)\n",
        "    return x_adv\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYCEQwmcrmH6"
      },
      "source": [
        "## Utils -- Attack\n",
        "\n",
        "* Recall\n",
        "    * ToTensor() can be seen as a function where $T(x) = x/255$.\n",
        "    * Normalize() can be seen as a function where $N(x) = (x-mean)/std$ where $mean$ and $std$ are constants.\n",
        "\n",
        "* Inverse function\n",
        "    * Inverse Normalize() can be seen as a function where $N^{-1}(x) = x*std+mean$ where $mean$ and $std$ are constants.\n",
        "    * Inverse ToTensor() can be seen as a function where $T^{-1}(x) = x*255$.\n",
        "\n",
        "* Special Noted\n",
        "    * ToTensor() will also convert the image from shape (height, width, channel) to shape (channel, height, width), so we also need to transpose the shape back to original shape.\n",
        "    * Since our dataloader samples a batch of data, what we need here is to transpose **(batch_size, channel, height, width)** back to **(batch_size, height, width, channel)** using np.transpose."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5X_9x-7ro_w"
      },
      "source": [
        "# perform adversarial attack and generate adversarial examples\n",
        "def gen_adv_examples(models, loader, attack, loss_fn):\n",
        "    for model in models:\n",
        "        model.eval()\n",
        "    adv_names = []\n",
        "    train_acc, train_loss = 0.0, 0.0\n",
        "    for i, (x, y) in enumerate(loader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        x_adv = attack(models, x, y, loss_fn) # obtain adversarial examples\n",
        "\n",
        "        yps = list()\n",
        "        for model in models:\n",
        "            yps.append(model(x_adv))\n",
        "        for index, yp in enumerate(yps):\n",
        "            if index == 0:\n",
        "                loss = loss_fn(yp, y) * p\n",
        "            else:\n",
        "                loss += loss_fn(yp, y) * p\n",
        "            train_acc += (yp.argmax(dim=1) == y).sum().item() * p\n",
        "        train_loss += loss.item() * x.shape[0]\n",
        "        # store adversarial examples\n",
        "        adv_ex = ((x_adv) * std + mean).clamp(0, 1) # to 0-1 scale\n",
        "        adv_ex = (adv_ex * 255).clamp(0, 255) # 0-255 scale\n",
        "        adv_ex = adv_ex.detach().cpu().data.numpy().round() # round to remove decimal part\n",
        "        adv_ex = adv_ex.transpose((0, 2, 3, 1)) # transpose (bs, C, H, W) back to (bs, H, W, C)\n",
        "        adv_examples = adv_ex if i == 0 else np.r_[adv_examples, adv_ex]\n",
        "    return adv_examples, train_acc / len(loader.dataset), train_loss / len(loader.dataset)\n",
        "\n",
        "# create directory which stores adversarial examples\n",
        "def create_dir(data_dir, adv_dir, adv_examples, adv_names):\n",
        "    if os.path.exists(adv_dir) is not True:\n",
        "        _ = shutil.copytree(data_dir, adv_dir)\n",
        "    for example, name in zip(adv_examples, adv_names):\n",
        "        im = Image.fromarray(example.astype(np.uint8)) # image pixel value should be unsigned int\n",
        "        im.save(os.path.join(adv_dir, name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_pMkmPytX3k"
      },
      "source": [
        "## Model / Loss Function\n",
        "\n",
        "Model list is available [here](https://github.com/osmr/imgclsmob/blob/master/pytorch/pytorchcv/model_provider.py). Please select models which has _cifar10 suffix. Some of the models cannot be accessed/loaded. You can safely skip them since TA's model will not use those kinds of models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwto8xbPtYzQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57532895-a0c7-42bd-e239-c8dc0ec93943"
      },
      "source": [
        "from pytorchcv.model_provider import get_model as ptcv_get_model\n",
        "\n",
        "# model = ptcv_get_model('resnet110_cifar10', pretrained=True).to(device)\n",
        "model_list = ['seresnet56_cifar10', 'msdnet22_cifar10', 'rir_cifar10', 'diapreresnet110_cifar10', 'fractalnet_cifar10',\n",
        "        'resdropresnet20_cifar10', 'wrn40_8_cifar10', 'densenet40_k12_cifar10', 'resnet110_cifar10', 'preresnet56_cifar10']\n",
        "# model_list = ['seresnet20_cifar10', 'diaresnet110_cifar10', 'preresnet20_cifar10', 'resnet20_cifar10', 'nin_cifar10',\n",
        "#         'diapreresnet20_cifar10', 'diapreresnet56_cifar10', 'sepreresnet110_cifar10', 'resnet56_cifar10', 'pyramidnet110_a48_cifar10']\n",
        "models = list()\n",
        "for model in model_list:\n",
        "    models.append(ptcv_get_model(model, pretrained=True).to(device))\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "benign_acc, benign_loss = epoch_benign(models, adv_loader, loss_fn)\n",
        "print(f'benign_acc = {benign_acc:.5f}, benign_loss = {benign_loss:.5f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading /root/.torch/models/diapreresnet20_cifar10-0642-14a1eb85.pth.zip from https://github.com/osmr/imgclsmob/releases/download/v0.0.343/diapreresnet20_cifar10-0642-14a1eb85.pth.zip...\n",
            "Downloading /root/.torch/models/diapreresnet56_cifar10-0483-41cae958.pth.zip from https://github.com/osmr/imgclsmob/releases/download/v0.0.343/diapreresnet56_cifar10-0483-41cae958.pth.zip...\n",
            "Downloading /root/.torch/models/sepreresnet110_cifar10-0454-418daea9.pth.zip from https://github.com/osmr/imgclsmob/releases/download/v0.0.379/sepreresnet110_cifar10-0454-418daea9.pth.zip...\n",
            "Downloading /root/.torch/models/resnet56_cifar10-0452-628c42a2.pth.zip from https://github.com/osmr/imgclsmob/releases/download/v0.0.163/resnet56_cifar10-0452-628c42a2.pth.zip...\n",
            "Downloading /root/.torch/models/pyramidnet110_a48_cifar10-0372-eb185645.pth.zip from https://github.com/osmr/imgclsmob/releases/download/v0.0.184/pyramidnet110_a48_cifar10-0372-eb185645.pth.zip...\n",
            "benign_acc = 0.94500, benign_loss = 0.20839\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uslb7GPchtMI"
      },
      "source": [
        "## FGSM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQwPTVUIhuTS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25381b2c-ab93-49b0-d9e7-2b9166cc3ab8"
      },
      "source": [
        "adv_examples, fgsm_acc, fgsm_loss = gen_adv_examples(models, adv_loader, fgsm, loss_fn)\n",
        "print(f'fgsm_acc = {fgsm_acc:.5f}, fgsm_loss = {fgsm_loss:.5f}')\n",
        "\n",
        "create_dir(root, 'fgsm', adv_examples, adv_names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fgsm_acc = 0.41300, fgsm_loss = 2.61361\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXw6p0A6shZm"
      },
      "source": [
        "## I-FGSM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUEsT06Iskt2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6f02dbb-26ec-4f6b-b5f2-8170572b4da3"
      },
      "source": [
        "# TODO: iterative fgsm attack\n",
        "adv_examples, ifgsm_acc, ifgsm_loss = gen_adv_examples(models, adv_loader, ifgsm, loss_fn)\n",
        "print(f'ifgsm_acc = {ifgsm_acc:.5f}, ifgsm_loss = {ifgsm_loss:.5f}')\n",
        "\n",
        "create_dir(root, 'ifgsm', adv_examples, adv_names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ifgsm_acc = 0.01100, ifgsm_loss = 14.45450\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQ-nYkkYexEE"
      },
      "source": [
        "## Compress the images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItRo_S0M264N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbbdefc6-b84a-4a5e-d4ab-3d10710d3a66"
      },
      "source": [
        "# %cd fgsm\n",
        "# !tar zcvf ../fgsm.tgz *\n",
        "# %cd ..\n",
        "\n",
        "%cd ifgsm\n",
        "!tar zcvf ../ifgsm.tgz *\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/ifgsm\n",
            "airplane/\n",
            "airplane/airplane10.png\n",
            "airplane/airplane20.png\n",
            "airplane/airplane12.png\n",
            "airplane/airplane17.png\n",
            "airplane/airplane14.png\n",
            "airplane/airplane6.png\n",
            "airplane/airplane1.png\n",
            "airplane/airplane5.png\n",
            "airplane/airplane4.png\n",
            "airplane/airplane7.png\n",
            "airplane/airplane2.png\n",
            "airplane/airplane16.png\n",
            "airplane/airplane18.png\n",
            "airplane/airplane3.png\n",
            "airplane/airplane19.png\n",
            "airplane/airplane8.png\n",
            "airplane/airplane9.png\n",
            "airplane/airplane15.png\n",
            "airplane/airplane11.png\n",
            "airplane/airplane13.png\n",
            "automobile/\n",
            "automobile/automobile19.png\n",
            "automobile/automobile5.png\n",
            "automobile/automobile18.png\n",
            "automobile/automobile8.png\n",
            "automobile/automobile3.png\n",
            "automobile/automobile16.png\n",
            "automobile/automobile11.png\n",
            "automobile/automobile7.png\n",
            "automobile/automobile10.png\n",
            "automobile/automobile15.png\n",
            "automobile/automobile20.png\n",
            "automobile/automobile13.png\n",
            "automobile/automobile17.png\n",
            "automobile/automobile1.png\n",
            "automobile/automobile4.png\n",
            "automobile/automobile6.png\n",
            "automobile/automobile14.png\n",
            "automobile/automobile9.png\n",
            "automobile/automobile12.png\n",
            "automobile/automobile2.png\n",
            "bird/\n",
            "bird/bird14.png\n",
            "bird/bird18.png\n",
            "bird/bird4.png\n",
            "bird/bird1.png\n",
            "bird/bird17.png\n",
            "bird/bird19.png\n",
            "bird/bird12.png\n",
            "bird/bird2.png\n",
            "bird/bird15.png\n",
            "bird/bird11.png\n",
            "bird/bird3.png\n",
            "bird/bird9.png\n",
            "bird/bird16.png\n",
            "bird/bird6.png\n",
            "bird/bird20.png\n",
            "bird/bird5.png\n",
            "bird/bird8.png\n",
            "bird/bird13.png\n",
            "bird/bird10.png\n",
            "bird/bird7.png\n",
            "cat/\n",
            "cat/cat15.png\n",
            "cat/cat19.png\n",
            "cat/cat1.png\n",
            "cat/cat17.png\n",
            "cat/cat18.png\n",
            "cat/cat2.png\n",
            "cat/cat20.png\n",
            "cat/cat3.png\n",
            "cat/cat8.png\n",
            "cat/cat13.png\n",
            "cat/cat5.png\n",
            "cat/cat6.png\n",
            "cat/cat11.png\n",
            "cat/cat4.png\n",
            "cat/cat16.png\n",
            "cat/cat14.png\n",
            "cat/cat12.png\n",
            "cat/cat7.png\n",
            "cat/cat9.png\n",
            "cat/cat10.png\n",
            "deer/\n",
            "deer/deer9.png\n",
            "deer/deer8.png\n",
            "deer/deer1.png\n",
            "deer/deer5.png\n",
            "deer/deer19.png\n",
            "deer/deer15.png\n",
            "deer/deer12.png\n",
            "deer/deer17.png\n",
            "deer/deer10.png\n",
            "deer/deer6.png\n",
            "deer/deer18.png\n",
            "deer/deer4.png\n",
            "deer/deer13.png\n",
            "deer/deer2.png\n",
            "deer/deer7.png\n",
            "deer/deer20.png\n",
            "deer/deer11.png\n",
            "deer/deer14.png\n",
            "deer/deer16.png\n",
            "deer/deer3.png\n",
            "dog/\n",
            "dog/dog11.png\n",
            "dog/dog2.png\n",
            "dog/dog5.png\n",
            "dog/dog1.png\n",
            "dog/dog14.png\n",
            "dog/dog3.png\n",
            "dog/dog10.png\n",
            "dog/dog15.png\n",
            "dog/dog12.png\n",
            "dog/dog8.png\n",
            "dog/dog6.png\n",
            "dog/dog17.png\n",
            "dog/dog20.png\n",
            "dog/dog9.png\n",
            "dog/dog4.png\n",
            "dog/dog16.png\n",
            "dog/dog13.png\n",
            "dog/dog19.png\n",
            "dog/dog18.png\n",
            "dog/dog7.png\n",
            "frog/\n",
            "frog/frog15.png\n",
            "frog/frog14.png\n",
            "frog/frog4.png\n",
            "frog/frog20.png\n",
            "frog/frog9.png\n",
            "frog/frog3.png\n",
            "frog/frog6.png\n",
            "frog/frog18.png\n",
            "frog/frog11.png\n",
            "frog/frog7.png\n",
            "frog/frog2.png\n",
            "frog/frog1.png\n",
            "frog/frog12.png\n",
            "frog/frog8.png\n",
            "frog/frog17.png\n",
            "frog/frog19.png\n",
            "frog/frog13.png\n",
            "frog/frog16.png\n",
            "frog/frog5.png\n",
            "frog/frog10.png\n",
            "horse/\n",
            "horse/horse11.png\n",
            "horse/horse8.png\n",
            "horse/horse10.png\n",
            "horse/horse1.png\n",
            "horse/horse15.png\n",
            "horse/horse3.png\n",
            "horse/horse13.png\n",
            "horse/horse7.png\n",
            "horse/horse20.png\n",
            "horse/horse18.png\n",
            "horse/horse17.png\n",
            "horse/horse6.png\n",
            "horse/horse5.png\n",
            "horse/horse4.png\n",
            "horse/horse12.png\n",
            "horse/horse19.png\n",
            "horse/horse14.png\n",
            "horse/horse9.png\n",
            "horse/horse16.png\n",
            "horse/horse2.png\n",
            "ship/\n",
            "ship/ship11.png\n",
            "ship/ship12.png\n",
            "ship/ship7.png\n",
            "ship/ship13.png\n",
            "ship/ship14.png\n",
            "ship/ship17.png\n",
            "ship/ship16.png\n",
            "ship/ship1.png\n",
            "ship/ship18.png\n",
            "ship/ship6.png\n",
            "ship/ship19.png\n",
            "ship/ship3.png\n",
            "ship/ship20.png\n",
            "ship/ship10.png\n",
            "ship/ship4.png\n",
            "ship/ship2.png\n",
            "ship/ship8.png\n",
            "ship/ship9.png\n",
            "ship/ship15.png\n",
            "ship/ship5.png\n",
            "truck/\n",
            "truck/truck12.png\n",
            "truck/truck13.png\n",
            "truck/truck8.png\n",
            "truck/truck9.png\n",
            "truck/truck3.png\n",
            "truck/truck4.png\n",
            "truck/truck19.png\n",
            "truck/truck20.png\n",
            "truck/truck11.png\n",
            "truck/truck10.png\n",
            "truck/truck15.png\n",
            "truck/truck7.png\n",
            "truck/truck16.png\n",
            "truck/truck5.png\n",
            "truck/truck14.png\n",
            "truck/truck2.png\n",
            "truck/truck1.png\n",
            "truck/truck6.png\n",
            "truck/truck17.png\n",
            "truck/truck18.png\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FM_S886kFd8"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FCuE2njkH1O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "0e9cd03c-f7ae-4359-d7aa-187f8391a7f8"
      },
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "# plt.figure(figsize=(10, 20))\n",
        "# cnt = 0\n",
        "# for i, cls_name in enumerate(classes):\n",
        "#     path = f'{cls_name}/{cls_name}1.png'\n",
        "#     # benign image\n",
        "#     cnt += 1\n",
        "#     plt.subplot(len(classes), 4, cnt)\n",
        "#     im = Image.open(f'./data/{path}')\n",
        "#     for model in models\n",
        "#     logit = model(transform(im).unsqueeze(0).to(device))[0]\n",
        "#     predict = logit.argmax(-1).item()\n",
        "#     prob = logit.softmax(-1)[predict].item()\n",
        "#     plt.title(f'benign: {cls_name}1.png\\n{classes[predict]}: {prob:.2%}')\n",
        "#     plt.axis('off')\n",
        "#     plt.imshow(np.array(im))\n",
        "#     # adversarial image\n",
        "#     cnt += 1\n",
        "#     plt.subplot(len(classes), 4, cnt)\n",
        "#     im = Image.open(f'./ifgsm/{path}')\n",
        "#     logit = model(transform(im).unsqueeze(0).to(device))[0]\n",
        "#     predict = logit.argmax(-1).item()\n",
        "#     prob = logit.softmax(-1)[predict].item()\n",
        "#     plt.title(f'adversarial: {cls_name}1.png\\n{classes[predict]}: {prob:.2%}')\n",
        "#     plt.axis('off')\n",
        "#     plt.imshow(np.array(im))\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-a12d3bc60eba>\"\u001b[0;36m, line \u001b[0;32m13\u001b[0m\n\u001b[0;31m    for model in models\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    }
  ]
}