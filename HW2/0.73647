{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"0.73647","provenance":[{"file_id":"https://github.com/ga642381/ML2021-Spring/blob/main/HW02/HW02-1.ipynb","timestamp":1616143146073}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"OYlaRwNu7ojq"},"source":["# **Homework 2-1 Phoneme Classification**"]},{"cell_type":"markdown","metadata":{"id":"emUd7uS7crTz"},"source":["## The DARPA TIMIT Acoustic-Phonetic Continuous Speech Corpus (TIMIT)\n","The TIMIT corpus of reading speech has been designed to provide speech data for the acquisition of acoustic-phonetic knowledge and for the development and evaluation of automatic speech recognition systems.\n","\n","This homework is a multiclass classification task, \n","we are going to train a deep neural network classifier to predict the phonemes for each frame from the speech corpus TIMIT.\n","\n","link: https://academictorrents.com/details/34e2b78745138186976cbc27939b1b34d18bd5b3"]},{"cell_type":"markdown","metadata":{"id":"KVUGfWTo7_Oj"},"source":["## Download Data\n","Download data from google drive, then unzip it.\n","\n","You should have `timit_11/train_11.npy`, `timit_11/train_label_11.npy`, and `timit_11/test_11.npy` after running this block.<br><br>\n","`timit_11/`\n","- `train_11.npy`: training data<br>\n","- `train_label_11.npy`: training label<br>\n","- `test_11.npy`:  testing data<br><br>\n","\n","**notes: if the google drive link is dead, you can download the data directly from Kaggle and upload it to the workspace**\n","\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OzkiMEcC3Foq","executionInfo":{"elapsed":1345,"status":"ok","timestamp":1616853883709,"user":{"displayName":"YDBX","photoUrl":"https://lh5.googleusercontent.com/-UMSb4ZzIN6M/AAAAAAAAAAI/AAAAAAAAHkc/DgzvjcH-KTA/s64/photo.jpg","userId":"15751498837825255117"},"user_tz":-480},"outputId":"211de8cb-0667-42bc-fa9e-e0a522eede50"},"source":["!gdown --id '1HPkcmQmFGu-3OknddKIa5dNDsR05lIQR' --output data.zip\n","!unzip data.zip\n","!ls "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Permission denied: https://drive.google.com/uc?id=1HPkcmQmFGu-3OknddKIa5dNDsR05lIQR\n","Maybe you need to change permission over 'Anyone with the link'?\n","unzip:  cannot find or open data.zip, data.zip.zip or data.zip.ZIP.\n","sample_data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N2mHg4TqtgSV","executionInfo":{"status":"ok","timestamp":1617448196615,"user_tz":-480,"elapsed":23890,"user":{"displayName":"YDBX","photoUrl":"https://lh5.googleusercontent.com/-UMSb4ZzIN6M/AAAAAAAAAAI/AAAAAAAAHkc/DgzvjcH-KTA/s64/photo.jpg","userId":"15751498837825255117"}},"outputId":"4c7a4c0b-32cc-49d8-a73d-21536f10cc8f"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_L_4anls8Drv"},"source":["## Preparing Data\n","Load the training and testing data from the `.npy` file (NumPy array)."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IJjLT8em-y9G","executionInfo":{"status":"ok","timestamp":1617448347374,"user_tz":-480,"elapsed":123894,"user":{"displayName":"YDBX","photoUrl":"https://lh5.googleusercontent.com/-UMSb4ZzIN6M/AAAAAAAAAAI/AAAAAAAAHkc/DgzvjcH-KTA/s64/photo.jpg","userId":"15751498837825255117"}},"outputId":"e74e3e40-76ea-4203-c95e-c139f6671cbc"},"source":["import numpy as np\n","\n","print('Loading data ...')\n","\n","data_root='./drive/MyDrive/timit_11/'\n","train = np.load(data_root + 'train_11.npy')\n","train_label = np.load(data_root + 'train_label_11.npy')\n","test = np.load(data_root + 'test_11.npy')\n","\n","print('Size of training data: {}'.format(train.shape))\n","print('Size of testing data: {}'.format(test.shape))"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Loading data ...\n","Size of training data: (1229932, 429)\n","Size of testing data: (451552, 429)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"us5XW_x6udZQ"},"source":["## Create Dataset"]},{"cell_type":"code","metadata":{"id":"Fjf5EcmJtf4e","executionInfo":{"status":"ok","timestamp":1617448372649,"user_tz":-480,"elapsed":5024,"user":{"displayName":"YDBX","photoUrl":"https://lh5.googleusercontent.com/-UMSb4ZzIN6M/AAAAAAAAAAI/AAAAAAAAHkc/DgzvjcH-KTA/s64/photo.jpg","userId":"15751498837825255117"}}},"source":["import torch\n","from torch.utils.data import Dataset\n","\n","class TIMITDataset(Dataset):\n","    def __init__(self, X, y=None):\n","        self.data = torch.from_numpy(X).float()\n","        if y is not None:\n","            y = y.astype(np.int)\n","            self.label = torch.LongTensor(y)\n","        else:\n","            self.label = None\n","\n","    def __getitem__(self, idx):\n","        if self.label is not None:\n","            return self.data[idx], self.label[idx]\n","        else:\n","            return self.data[idx]\n","\n","    def __len__(self):\n","        return len(self.data)\n"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"otIC6WhGeh9v"},"source":["Split the labeled data into a training set and a validation set, you can modify the variable `VAL_RATIO` to change the ratio of validation data."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sYqi_lAuvC59","executionInfo":{"status":"ok","timestamp":1617448374654,"user_tz":-480,"elapsed":734,"user":{"displayName":"YDBX","photoUrl":"https://lh5.googleusercontent.com/-UMSb4ZzIN6M/AAAAAAAAAAI/AAAAAAAAHkc/DgzvjcH-KTA/s64/photo.jpg","userId":"15751498837825255117"}},"outputId":"28ce6e8c-f587-4c4a-e0a9-74657b3d5843"},"source":["VAL_RATIO = 0.2\n","\n","percent = int(train.shape[0] * (1 - VAL_RATIO))\n","train_x, train_y, val_x, val_y = train[:percent], train_label[:percent], train[percent:], train_label[percent:]\n","print('Size of training set: {}'.format(train_x.shape))\n","print('Size of validation set: {}'.format(val_x.shape))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Size of training set: (983945, 429)\n","Size of validation set: (245987, 429)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nbCfclUIgMTX"},"source":["Create a data loader from the dataset, feel free to tweak the variable `BATCH_SIZE` here."]},{"cell_type":"code","metadata":{"id":"RUCbQvqJurYc","executionInfo":{"status":"ok","timestamp":1617448380513,"user_tz":-480,"elapsed":3377,"user":{"displayName":"YDBX","photoUrl":"https://lh5.googleusercontent.com/-UMSb4ZzIN6M/AAAAAAAAAAI/AAAAAAAAHkc/DgzvjcH-KTA/s64/photo.jpg","userId":"15751498837825255117"}}},"source":["BATCH_SIZE = 64\n","\n","from torch.utils.data import DataLoader\n","\n","train_set = TIMITDataset(train_x, train_y)\n","val_set = TIMITDataset(val_x, val_y)\n","train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True) #only shuffle the training data\n","val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_SY7X0lUgb50"},"source":["Cleanup the unneeded variables to save memory.<br>\n","\n","**notes: if you need to use these variables later, then you may remove this block or clean up unneeded variables later<br>the data size is quite huge, so be aware of memory usage in colab**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y8rzkGraeYeN","executionInfo":{"status":"ok","timestamp":1617448382335,"user_tz":-480,"elapsed":617,"user":{"displayName":"YDBX","photoUrl":"https://lh5.googleusercontent.com/-UMSb4ZzIN6M/AAAAAAAAAAI/AAAAAAAAHkc/DgzvjcH-KTA/s64/photo.jpg","userId":"15751498837825255117"}},"outputId":"5a1e31b6-c142-4f1a-91c3-558f5967ef1d"},"source":["import gc\n","\n","del train, train_label, train_x, train_y, val_x, val_y\n","gc.collect()"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["261"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"IRqKNvNZwe3V"},"source":["## Create Model"]},{"cell_type":"markdown","metadata":{"id":"FYr1ng5fh9pA"},"source":["Define model architecture, you are encouraged to change and experiment with the model architecture."]},{"cell_type":"code","metadata":{"id":"lbZrwT6Ny0XL","executionInfo":{"status":"ok","timestamp":1617448386793,"user_tz":-480,"elapsed":1321,"user":{"displayName":"YDBX","photoUrl":"https://lh5.googleusercontent.com/-UMSb4ZzIN6M/AAAAAAAAAAI/AAAAAAAAHkc/DgzvjcH-KTA/s64/photo.jpg","userId":"15751498837825255117"}}},"source":["import torch\n","import torch.nn as nn\n","\n","class Classifier(nn.Module):\n","    def __init__(self):\n","        super(Classifier, self).__init__()\n","        self.net = nn.Sequential(\n","            nn.BatchNorm1d(429),\n","            nn.Linear(429, 1024),            \n","            nn.Sigmoid(),\n","            nn.ReLU(),\n","            nn.Dropout(0.2),\n","            nn.Linear(1024, 512),\n","            nn.Sigmoid(),\n","            nn.ReLU(),\n","            nn.Dropout(0.2),\n","            nn.Linear(512, 128),\n","            nn.Sigmoid(),\n","            nn.ReLU(),\n","            nn.Dropout(0.2),\n","            nn.Linear(128, 39),\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VRYciXZvPbYh"},"source":["## Training"]},{"cell_type":"code","metadata":{"id":"y114Vmm3Ja6o","executionInfo":{"status":"ok","timestamp":1617448390614,"user_tz":-480,"elapsed":1542,"user":{"displayName":"YDBX","photoUrl":"https://lh5.googleusercontent.com/-UMSb4ZzIN6M/AAAAAAAAAAI/AAAAAAAAHkc/DgzvjcH-KTA/s64/photo.jpg","userId":"15751498837825255117"}}},"source":["#check device\n","def get_device():\n","  return 'cuda' if torch.cuda.is_available() else 'cpu'"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sEX-yjHjhGuH"},"source":["Fix random seeds for reproducibility."]},{"cell_type":"code","metadata":{"id":"88xPiUnm0tAd","executionInfo":{"status":"ok","timestamp":1617448392588,"user_tz":-480,"elapsed":1171,"user":{"displayName":"YDBX","photoUrl":"https://lh5.googleusercontent.com/-UMSb4ZzIN6M/AAAAAAAAAAI/AAAAAAAAHkc/DgzvjcH-KTA/s64/photo.jpg","userId":"15751498837825255117"}}},"source":["# fix random seed\n","def same_seeds(seed):\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed)\n","        torch.cuda.manual_seed_all(seed)  \n","    np.random.seed(seed)  \n","    torch.backends.cudnn.benchmark = False\n","    torch.backends.cudnn.deterministic = True"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KbBcBXkSp6RA"},"source":["Feel free to change the training parameters here."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QTp3ZXg1yO9Y","executionInfo":{"status":"ok","timestamp":1617448407442,"user_tz":-480,"elapsed":11539,"user":{"displayName":"YDBX","photoUrl":"https://lh5.googleusercontent.com/-UMSb4ZzIN6M/AAAAAAAAAAI/AAAAAAAAHkc/DgzvjcH-KTA/s64/photo.jpg","userId":"15751498837825255117"}},"outputId":"8f4a14d9-9485-4764-aaf6-fc5d3eba0d3b"},"source":["# fix random seed for reproducibility\n","same_seeds(0)\n","\n","# get device \n","device = get_device()\n","print(f'DEVICE: {device}')\n","\n","# training parameters\n","num_epoch = 20               # number of training epoch\n","#40epoch: 20epoch0.001 20epoch 0.0001\n","learning_rate = 0.001       # learning rate\n","wd = 1e-5\n","\n","# the path where checkpoint saved\n","model_path = './drive/MyDrive/checkpoint/model.ckpt'\n","\n","# create model, define a loss function, and optimizer\n","model = Classifier().to(device)\n","criterion = nn.CrossEntropyLoss() \n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=wd)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["DEVICE: cuda\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"CdMWsBs7zzNs","executionInfo":{"status":"ok","timestamp":1617449302319,"user_tz":-480,"elapsed":901848,"user":{"displayName":"YDBX","photoUrl":"https://lh5.googleusercontent.com/-UMSb4ZzIN6M/AAAAAAAAAAI/AAAAAAAAHkc/DgzvjcH-KTA/s64/photo.jpg","userId":"15751498837825255117"}},"outputId":"6bd473a8-113b-42fc-8672-6b3b82a9958f"},"source":["# start training\n","\n","best_acc = 0.0\n","for epoch in range(num_epoch):\n","    train_acc = 0.0\n","    train_loss = 0.0\n","    val_acc = 0.0\n","    val_loss = 0.0\n","\n","    # training\n","    model.train() # set the model to training mode\n","    for i, data in enumerate(train_loader):\n","        inputs, labels = data\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(inputs) \n","        batch_loss = criterion(outputs, labels)\n","        _, train_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n","        batch_loss.backward() \n","        optimizer.step() \n","\n","        train_acc += (train_pred.cpu() == labels.cpu()).sum().item()\n","        train_loss += batch_loss.item()\n","\n","    # validation\n","    if len(val_set) > 0:\n","        model.eval() # set the model to evaluation mode\n","        with torch.no_grad():\n","            for i, data in enumerate(val_loader):\n","                inputs, labels = data\n","                inputs, labels = inputs.to(device), labels.to(device)\n","                outputs = model(inputs)\n","                batch_loss = criterion(outputs, labels) \n","                _, val_pred = torch.max(outputs, 1) \n","            \n","                val_acc += (val_pred.cpu() == labels.cpu()).sum().item() # get the index of the class with the highest probability\n","                val_loss += batch_loss.item()\n","\n","            print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f} | Val Acc: {:3.6f} loss: {:3.6f}'.format(\n","                epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader), val_acc/len(val_set), val_loss/len(val_loader)\n","            ))\n","\n","            # if the model improves, save a checkpoint at this epoch\n","            if val_acc > best_acc:\n","                best_acc = val_acc\n","                torch.save(model.state_dict(), model_path)\n","                print('saving model with acc {:.3f}'.format(best_acc/len(val_set)))\n","    else:\n","        print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f}'.format(\n","            epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader)\n","        ))\n","\n","# if not validating, save the last epoch\n","if len(val_set) == 0:\n","    torch.save(model.state_dict(), model_path)\n","    print('saving model at last epoch')\n"],"execution_count":11,"outputs":[{"output_type":"stream","text":["[001/020] Train Acc: 0.563947 Loss: 1.425871 | Val Acc: 0.670999 loss: 1.036273\n","saving model with acc 0.671\n","[002/020] Train Acc: 0.642614 Loss: 1.131697 | Val Acc: 0.694581 loss: 0.946926\n","saving model with acc 0.695\n","[003/020] Train Acc: 0.662144 Loss: 1.058298 | Val Acc: 0.702688 loss: 0.907850\n","saving model with acc 0.703\n","[004/020] Train Acc: 0.673622 Loss: 1.016469 | Val Acc: 0.711969 loss: 0.883210\n","saving model with acc 0.712\n","[005/020] Train Acc: 0.680788 Loss: 0.988745 | Val Acc: 0.717050 loss: 0.860886\n","saving model with acc 0.717\n","[006/020] Train Acc: 0.687090 Loss: 0.967757 | Val Acc: 0.721953 loss: 0.847986\n","saving model with acc 0.722\n","[007/020] Train Acc: 0.691495 Loss: 0.951420 | Val Acc: 0.722172 loss: 0.841411\n","saving model with acc 0.722\n","[008/020] Train Acc: 0.695347 Loss: 0.937821 | Val Acc: 0.724351 loss: 0.835864\n","saving model with acc 0.724\n","[009/020] Train Acc: 0.698032 Loss: 0.927019 | Val Acc: 0.726530 loss: 0.826764\n","saving model with acc 0.727\n","[010/020] Train Acc: 0.699890 Loss: 0.918862 | Val Acc: 0.728526 loss: 0.819949\n","saving model with acc 0.729\n","[011/020] Train Acc: 0.702061 Loss: 0.912353 | Val Acc: 0.729949 loss: 0.816218\n","saving model with acc 0.730\n","[012/020] Train Acc: 0.703852 Loss: 0.904974 | Val Acc: 0.730600 loss: 0.812491\n","saving model with acc 0.731\n","[013/020] Train Acc: 0.705020 Loss: 0.900046 | Val Acc: 0.730518 loss: 0.812092\n","[014/020] Train Acc: 0.706140 Loss: 0.894970 | Val Acc: 0.730925 loss: 0.811300\n","saving model with acc 0.731\n","[015/020] Train Acc: 0.707673 Loss: 0.891126 | Val Acc: 0.732815 loss: 0.807410\n","saving model with acc 0.733\n","[016/020] Train Acc: 0.709125 Loss: 0.886120 | Val Acc: 0.730750 loss: 0.809927\n","[017/020] Train Acc: 0.709104 Loss: 0.883117 | Val Acc: 0.733409 loss: 0.805102\n","saving model with acc 0.733\n","[018/020] Train Acc: 0.710812 Loss: 0.879883 | Val Acc: 0.730282 loss: 0.811918\n","[019/020] Train Acc: 0.711077 Loss: 0.878055 | Val Acc: 0.731953 loss: 0.805803\n","[020/020] Train Acc: 0.711842 Loss: 0.874912 | Val Acc: 0.733506 loss: 0.802383\n","saving model with acc 0.734\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ehgacL0Eapj_","executionInfo":{"status":"ok","timestamp":1617449339821,"user_tz":-480,"elapsed":830,"user":{"displayName":"YDBX","photoUrl":"https://lh5.googleusercontent.com/-UMSb4ZzIN6M/AAAAAAAAAAI/AAAAAAAAHkc/DgzvjcH-KTA/s64/photo.jpg","userId":"15751498837825255117"}}},"source":["# training parameters\n","learning_rate = 0.0001       # learning rate\n","\n","# create model, define a loss function, and optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=wd)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qAZeKJ-jaqCR","executionInfo":{"status":"ok","timestamp":1617450233746,"user_tz":-480,"elapsed":892318,"user":{"displayName":"YDBX","photoUrl":"https://lh5.googleusercontent.com/-UMSb4ZzIN6M/AAAAAAAAAAI/AAAAAAAAHkc/DgzvjcH-KTA/s64/photo.jpg","userId":"15751498837825255117"}},"outputId":"763a9c1a-7b07-4562-d651-72da1cbce63d"},"source":["# start training\n","\n","best_acc = 0.0\n","for epoch in range(num_epoch):\n","    train_acc = 0.0\n","    train_loss = 0.0\n","    val_acc = 0.0\n","    val_loss = 0.0\n","\n","    # training\n","    model.train() # set the model to training mode\n","    for i, data in enumerate(train_loader):\n","        inputs, labels = data\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(inputs) \n","        batch_loss = criterion(outputs, labels)\n","        _, train_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n","        batch_loss.backward() \n","        optimizer.step() \n","\n","        train_acc += (train_pred.cpu() == labels.cpu()).sum().item()\n","        train_loss += batch_loss.item()\n","\n","    # validation\n","    if len(val_set) > 0:\n","        model.eval() # set the model to evaluation mode\n","        with torch.no_grad():\n","            for i, data in enumerate(val_loader):\n","                inputs, labels = data\n","                inputs, labels = inputs.to(device), labels.to(device)\n","                outputs = model(inputs)\n","                batch_loss = criterion(outputs, labels) \n","                _, val_pred = torch.max(outputs, 1) \n","            \n","                val_acc += (val_pred.cpu() == labels.cpu()).sum().item() # get the index of the class with the highest probability\n","                val_loss += batch_loss.item()\n","\n","            print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f} | Val Acc: {:3.6f} loss: {:3.6f}'.format(\n","                epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader), val_acc/len(val_set), val_loss/len(val_loader)\n","            ))\n","\n","            # if the model improves, save a checkpoint at this epoch\n","            if val_acc > best_acc:\n","                best_acc = val_acc\n","                torch.save(model.state_dict(), model_path)\n","                print('saving model with acc {:.3f}'.format(best_acc/len(val_set)))\n","    else:\n","        print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f}'.format(\n","            epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader)\n","        ))\n","\n","# if not validating, save the last epoch\n","if len(val_set) == 0:\n","    torch.save(model.state_dict(), model_path)\n","    print('saving model at last epoch')"],"execution_count":13,"outputs":[{"output_type":"stream","text":["[001/020] Train Acc: 0.733925 Loss: 0.799694 | Val Acc: 0.747125 loss: 0.757892\n","saving model with acc 0.747\n","[002/020] Train Acc: 0.740877 Loss: 0.776751 | Val Acc: 0.748365 loss: 0.752487\n","saving model with acc 0.748\n","[003/020] Train Acc: 0.742713 Loss: 0.768942 | Val Acc: 0.749349 loss: 0.747817\n","saving model with acc 0.749\n","[004/020] Train Acc: 0.745302 Loss: 0.760646 | Val Acc: 0.749706 loss: 0.748838\n","saving model with acc 0.750\n","[005/020] Train Acc: 0.746379 Loss: 0.756325 | Val Acc: 0.750019 loss: 0.746868\n","saving model with acc 0.750\n","[006/020] Train Acc: 0.746188 Loss: 0.754488 | Val Acc: 0.749556 loss: 0.749687\n","[007/020] Train Acc: 0.747725 Loss: 0.750886 | Val Acc: 0.749662 loss: 0.748278\n","[008/020] Train Acc: 0.748119 Loss: 0.749274 | Val Acc: 0.751450 loss: 0.743272\n","saving model with acc 0.751\n","[009/020] Train Acc: 0.748893 Loss: 0.746317 | Val Acc: 0.750406 loss: 0.744985\n","[010/020] Train Acc: 0.749998 Loss: 0.744212 | Val Acc: 0.751133 loss: 0.743832\n","[011/020] Train Acc: 0.749482 Loss: 0.743197 | Val Acc: 0.750495 loss: 0.745954\n","[012/020] Train Acc: 0.750533 Loss: 0.740963 | Val Acc: 0.750869 loss: 0.746754\n","[013/020] Train Acc: 0.750453 Loss: 0.740054 | Val Acc: 0.751694 loss: 0.742266\n","saving model with acc 0.752\n","[014/020] Train Acc: 0.750997 Loss: 0.737975 | Val Acc: 0.750495 loss: 0.745623\n","[015/020] Train Acc: 0.751649 Loss: 0.736821 | Val Acc: 0.751682 loss: 0.741519\n","[016/020] Train Acc: 0.751489 Loss: 0.735221 | Val Acc: 0.752129 loss: 0.742273\n","saving model with acc 0.752\n","[017/020] Train Acc: 0.752650 Loss: 0.733417 | Val Acc: 0.752064 loss: 0.742368\n","[018/020] Train Acc: 0.752846 Loss: 0.732412 | Val Acc: 0.752292 loss: 0.740016\n","saving model with acc 0.752\n","[019/020] Train Acc: 0.752210 Loss: 0.731923 | Val Acc: 0.751515 loss: 0.744857\n","[020/020] Train Acc: 0.753380 Loss: 0.730248 | Val Acc: 0.752007 loss: 0.744545\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1Hi7jTn3PX-m"},"source":["## Testing"]},{"cell_type":"markdown","metadata":{"id":"NfUECMFCn5VG"},"source":["Create a testing dataset, and load model from the saved checkpoint."]},{"cell_type":"code","metadata":{"id":"1PKjtAScPWtr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617450266089,"user_tz":-480,"elapsed":2052,"user":{"displayName":"YDBX","photoUrl":"https://lh5.googleusercontent.com/-UMSb4ZzIN6M/AAAAAAAAAAI/AAAAAAAAHkc/DgzvjcH-KTA/s64/photo.jpg","userId":"15751498837825255117"}},"outputId":"00f1cd7a-aa74-473e-f00b-d5b0bf2ba06f"},"source":["# create testing dataset\n","test_set = TIMITDataset(test, None)\n","test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n","\n","# create model and load weights from checkpoint\n","model = Classifier().to(device)\n","model.load_state_dict(torch.load(model_path))"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"940TtCCdoYd0"},"source":["Make prediction."]},{"cell_type":"code","metadata":{"id":"84HU5GGjPqR0","executionInfo":{"status":"ok","timestamp":1617450270862,"user_tz":-480,"elapsed":6810,"user":{"displayName":"YDBX","photoUrl":"https://lh5.googleusercontent.com/-UMSb4ZzIN6M/AAAAAAAAAAI/AAAAAAAAHkc/DgzvjcH-KTA/s64/photo.jpg","userId":"15751498837825255117"}}},"source":["predict = []\n","model.eval() # set the model to evaluation mode\n","with torch.no_grad():\n","    for i, data in enumerate(test_loader):\n","        inputs = data\n","        inputs = inputs.to(device)\n","        outputs = model(inputs)\n","        _, test_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n","\n","        for y in test_pred.cpu().numpy():\n","            predict.append(y)"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AWDf_C-omElb"},"source":["Write prediction to a CSV file.\n","\n","After finish running this block, download the file `prediction.csv` from the files section on the left-hand side and submit it to Kaggle."]},{"cell_type":"code","metadata":{"id":"GuljYSPHcZir","executionInfo":{"status":"ok","timestamp":1617450271294,"user_tz":-480,"elapsed":7237,"user":{"displayName":"YDBX","photoUrl":"https://lh5.googleusercontent.com/-UMSb4ZzIN6M/AAAAAAAAAAI/AAAAAAAAHkc/DgzvjcH-KTA/s64/photo.jpg","userId":"15751498837825255117"}}},"source":["with open('prediction.csv', 'w') as f:\n","    f.write('Id,Class\\n')\n","    for i, y in enumerate(predict):\n","        f.write('{},{}\\n'.format(i, y))"],"execution_count":16,"outputs":[]}]}